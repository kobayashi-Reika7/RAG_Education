【2026/02/13 00:00:00】
今日のログ2026_02_13.md を新規作成し、RAGを起動してください

【2026/02/13 01:00:00】
chunk_id: arimaが表示されないようにchunk_id: kusatsuだけを表示
- 質問文から温泉地名を検出し、該当チャンクのみに絞り込むメタデータフィルタリングを実装
- onsen_rag.py: LOCATION_KEYWORDS定義、_detect_locationメソッド追加
- onsen_rag.py: load_from_data_folder / load_json_chunks に location メタデータ追加
- onsen_rag.py: query / search_chunks にChromaDB whereフィルタ適用

【2026/02/13 02:00:00】
ハイブリッド検索を統合
- onsen_rag.py: BM25キーワード検索 + セマンティック検索をRRF（Reciprocal Rank Fusion）で統合
- __init__: semantic_weight パラメータ追加、self.documents（BM25用）保持
- _hybrid_search(): 温泉地フィルタ + セマンティック + BM25 → RRFスコアで統合
- query / search_chunks を _hybrid_search 経由に変更
- EnsembleRetriever不要（依存パッケージ問題を回避して自前実装）

【2026/02/13 03:00:00】
精度向上: Re-ranking実装
- onsen_rag.py: CrossEncoder（ms-marco-MiniLM-L-6-v2）をRe-rankingとして統合
- __init__: CrossEncoder初期化、initial_k/final_kパラメータ追加
- _rerank(): CrossEncoderで文書を再スコアリングし上位のみ返す
- _hybrid_search: 初期検索(initial_k=10) → RRF統合 → Re-ranking → 上位(final_k=3)のパイプライン
- 検索精度向上: ハイブリッド検索 + Re-rankingの2段階精度向上

【2026/02/13 04:00:00】
3段階検索パイプライン実装（類似度検索・スコアリング → LLM候補抽出 → 最終選択・スコア統合）
- _rerank: CrossEncoderスコアを保持して返すように変更
- _llm_extract_candidates: LLMが各候補の関連度を0〜10で評価し上位5件を抽出
- _final_selection: CEスコア×0.4 + LLMスコア×0.6 で統合し最終top_k件を選定
- _hybrid_search: 3段階パイプラインに全面改修
  Step1: セマンティック+BM25→RRF統合→CrossEncoderスコアリング
  Step2: LLM候補抽出（上位5件）
  Step3: スコア統合で最終選定

【2026/02/13 10:00:00】
サーバー接続が遅い根本的な原因を改善
- 原因分析: 5つのボトルネックを特定（ChromaDB毎回再構築、LLM接続テスト、HuggingFace HTTPチェック、BM25毎回再構築、LLM 2回呼び出し）
- ChromaDB永続化: chroma_onsen_db/ にデータハッシュ付きで永続化、変更がなければディスクからロード
  - _compute_data_hash, _load_cached_vectorstore, _save_data_hash メソッド追加
- LLM接続テスト廃止: 起動時 llm.invoke("test") の10秒タイムアウト+APIコールを削除
- HuggingFaceオフラインモード: HF_HUB_OFFLINE/TRANSFORMERS_OFFLINE を import 前に設定
  - HuggingFaceEmbeddings, CrossEncoder に local_files_only=True を追加
- BM25キャッシュ: _build_bm25_cache() で起動時に全文書+温泉地別BM25を事前構築
  - _bm25_all, _bm25_by_location を _hybrid_search から直接参照
- LLM候補抽出最適化: 候補数≤final_k+2件の場合LLM評価をスキップして高速化

【2026/02/13 11:00:00】
RAG改善8項目を一括実装

■ すぐやるべき（効果大・工数小）
1-1: CrossEncoder日本語モデル変更
  - ms-marco-MiniLM-L-6-v2（英語専用）→ mmarco-mMiniLMv2-L12-H384-v1（14言語対応）
  - mMARCO（多言語MS MARCO）で学習済み。日本語Re-ranking精度が大幅向上

2-2: クエリキャッシュ実装
  - OrderedDictベースのLRUキャッシュ（maxsize=128, TTL=5分）
  - 同一質問の重複LLM呼び出しを完全回避（キャッシュヒット時は即座に返却）
  - _cache_key, _get_from_cache, _put_to_cache メソッド追加

4-1/4-2: メタデータ型統一
  - arima_chunks.json: category(配列→文字列), area(配列→文字列), keywords→tags にリネーム
  - kusatsu_chunks.json の型と完全統一（ChromaDB メタデータの互換性向上）

3-1: FastAPI lifespan対応
  - @app.on_event("startup")（非推奨）→ @asynccontextmanager lifespan に移行
  - app.state.rag_system / app.state.support_bot でグローバル変数を廃止
  - shutdown時のリソース解放ロジック追加

■ 次にやるべき（効果大・工数中）
2-1: 応答時間計測
  - AnswerResponse に response_time_ms フィールドを追加
  - 各リクエストの処理時間を計測して返却

1-3: 信頼度閾値設定
  - CONFIDENCE_THRESHOLD = 0.01（CrossEncoderスコア基準）
  - 閾値未満の候補を _final_selection で除外
  - 全候補が閾値以下の場合は空リストを返し「該当情報なし」に

5-1/5-2: セキュリティ基盤
  - CORS: allow_origins=["*"] → 環境変数 CORS_ORIGINS で制御可能に
  - allow_methods を GET/POST のみに制限
  - 入力サニタイズ: Pydantic field_validator で制御文字除去・500文字制限
  - エラーメッセージ: 内部詳細を隠蔽し、ユーザー向けメッセージに統一

3-2: クラス分割
  - src/llm_factory.py を新規作成（LLM初期化ロジックを共通モジュールに抽出）
  - OnsenRAG._init_llm() → from src.llm_factory import create_llm に置換
