【2026/02/13 00:00:00】
今日のログ2026_02_13.md を新規作成し、RAGを起動してください

【2026/02/13 01:00:00】
chunk_id: arimaが表示されないようにchunk_id: kusatsuだけを表示
- 質問文から温泉地名を検出し、該当チャンクのみに絞り込むメタデータフィルタリングを実装
- onsen_rag.py: LOCATION_KEYWORDS定義、_detect_locationメソッド追加
- onsen_rag.py: load_from_data_folder / load_json_chunks に location メタデータ追加
- onsen_rag.py: query / search_chunks にChromaDB whereフィルタ適用

【2026/02/13 02:00:00】
ハイブリッド検索を統合
- onsen_rag.py: BM25キーワード検索 + セマンティック検索をRRF（Reciprocal Rank Fusion）で統合
- __init__: semantic_weight パラメータ追加、self.documents（BM25用）保持
- _hybrid_search(): 温泉地フィルタ + セマンティック + BM25 → RRFスコアで統合
- query / search_chunks を _hybrid_search 経由に変更
- EnsembleRetriever不要（依存パッケージ問題を回避して自前実装）

【2026/02/13 03:00:00】
精度向上: Re-ranking実装
- onsen_rag.py: CrossEncoder（ms-marco-MiniLM-L-6-v2）をRe-rankingとして統合
- __init__: CrossEncoder初期化、initial_k/final_kパラメータ追加
- _rerank(): CrossEncoderで文書を再スコアリングし上位のみ返す
- _hybrid_search: 初期検索(initial_k=10) → RRF統合 → Re-ranking → 上位(final_k=3)のパイプライン
- 検索精度向上: ハイブリッド検索 + Re-rankingの2段階精度向上

【2026/02/13 04:00:00】
3段階検索パイプライン実装（類似度検索・スコアリング → LLM候補抽出 → 最終選択・スコア統合）
- _rerank: CrossEncoderスコアを保持して返すように変更
- _llm_extract_candidates: LLMが各候補の関連度を0〜10で評価し上位5件を抽出
- _final_selection: CEスコア×0.4 + LLMスコア×0.6 で統合し最終top_k件を選定
- _hybrid_search: 3段階パイプラインに全面改修
  Step1: セマンティック+BM25→RRF統合→CrossEncoderスコアリング
  Step2: LLM候補抽出（上位5件）
  Step3: スコア統合で最終選定

【2026/02/13 10:00:00】
サーバー接続が遅い根本的な原因を改善
- 原因分析: 5つのボトルネックを特定（ChromaDB毎回再構築、LLM接続テスト、HuggingFace HTTPチェック、BM25毎回再構築、LLM 2回呼び出し）
- ChromaDB永続化: chroma_onsen_db/ にデータハッシュ付きで永続化、変更がなければディスクからロード
  - _compute_data_hash, _load_cached_vectorstore, _save_data_hash メソッド追加
- LLM接続テスト廃止: 起動時 llm.invoke("test") の10秒タイムアウト+APIコールを削除
- HuggingFaceオフラインモード: HF_HUB_OFFLINE/TRANSFORMERS_OFFLINE を import 前に設定
  - HuggingFaceEmbeddings, CrossEncoder に local_files_only=True を追加
- BM25キャッシュ: _build_bm25_cache() で起動時に全文書+温泉地別BM25を事前構築
  - _bm25_all, _bm25_by_location を _hybrid_search から直接参照
- LLM候補抽出最適化: 候補数≤final_k+2件の場合LLM評価をスキップして高速化
